{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "# ===============================\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# ===============================\n",
    "# ì‹œê°í™”\n",
    "# ===============================\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# ===============================\n",
    "# ë°ì´í„° ì²˜ë¦¬\n",
    "# ===============================\n",
    "import pandas as pd\\\n",
    "\n",
    "# ===============================\n",
    "# matplotlib í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "# ===============================\n",
    "font_path = \"C:/Windows/Fonts/malgun.ttf\"\n",
    "\n",
    "if not os.path.exists(font_path):\n",
    "    raise FileNotFoundError(\"âŒ malgun.ttf í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "font_name = mpl.font_manager.FontProperties(fname=font_path).get_name()\n",
    "mpl.rcParams[\"font.family\"] = font_name\n",
    "mpl.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./news_data/keyword_merged.csv\")\n",
    "\n",
    "# í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸\n",
    "required_cols = [\"ê²€ìƒ‰ì–´\", \"í‚¤ì›Œë“œ\"]\n",
    "for col in required_cols:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"[ERROR] '{col}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## í•¨ìˆ˜ ì„ ì–¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 2. UI helper í•¨ìˆ˜\n",
    "#############################################\n",
    "def format_search_query(search_word):\n",
    "    \"\"\"\n",
    "    ë°˜ë„ì²´_ì „ë§_20231201_20231231\n",
    "    â†’ ê²€ìƒ‰ì–´: ë°˜ë„ì²´ ì „ë§ (2023.12.01 ~ 2023.12.31)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        industry, keyword, start, end = search_word.split(\"_\")\n",
    "        return f\"ê²€ìƒ‰ì–´: {industry} {keyword} ({start[:4]}.{start[4:6]}.{start[6:]} ~ {end[:4]}.{end[4:6]}.{end[6:]})\"\n",
    "    except:\n",
    "        return f\"ê²€ìƒ‰ì–´: {search_word}\"\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 3. í† í° ì •ê·œí™” (ğŸ”¥ ëª¨ë“  ë¹„êµ ê¸°ì¤€)\n",
    "#############################################\n",
    "def normalize_token(token: str) -> str:\n",
    "    token = token.lower()\n",
    "    token = re.sub(r\"[^\\wê°€-í£]\", \"\", token)\n",
    "    return token\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 4. ë¶ˆìš©ì–´ ì •ì˜\n",
    "#############################################\n",
    "BASE_STOPWORDS = {\n",
    "    # =========================\n",
    "    # 1. ì¼ë°˜ ê¸°ëŠ¥ì–´ / ì¡°ì‚¬\n",
    "    # =========================\n",
    "    \"ê²ƒ\",\"ìˆ˜\",\"ë“±\",\"ë°\",\"ëŒ€í•œ\",\"ìœ„í•œ\",\"ê´€ë ¨\",\"ê²½ìš°\",\"ìˆ˜ì¤€\",\"ì¤‘ì‹¬\",\n",
    "    \"í†µí•´\",\"ëŒ€í•´\",\"ë¹„í•´\",\"ë”°ë¼\",\"ì´ìƒ\",\"ì´í•˜\",\"ë‚´ì™¸\",\"ì „ë°˜\",\n",
    "    \"ë¶€ë¶„\",\"ì¸¡ë©´\",\"ë°©ë©´\",\"ì´ìœ \",\"ë°°ê²½\",\"ê³¼ì •\",\"ë°©ì•ˆ\",\"ë‚´ìš©\",\n",
    "\n",
    "    # =========================\n",
    "    # 2. ë‰´ìŠ¤Â·ë¦¬í¬íŠ¸ ê´€ìš©ì–´\n",
    "    # =========================\n",
    "    \"ì „ë§\",\"í˜„í™©\",\"ë™í–¥\",\"ë¶„ì„\",\"ë³´ê³ \",\"ë³´ê³ ì„œ\",\n",
    "    \"ì‚°ì—…\",\"ê¸°ì—…\",\"ê¸°ìˆ \",\"ì‹œì¥\",\"êµ¬ì¡°\",\"í™˜ê²½\",\"ì˜í–¥\",\n",
    "    \"í™•ëŒ€\",\"ê°•í™”\",\"ì¶”ì§„\",\"ëŒ€ì‘\",\"í™œìš©\",\"ê¸°ë°˜\",\"ê°€ëŠ¥ì„±\",\n",
    "    \"í™•ë³´\",\"ê°•ì¡°\",\"ì„¤ëª…\",\"ì–¸ê¸‰\",\"ì§€ì \",\"í‰ê°€\",\"í™•ì¸\",\n",
    "    \"ì˜ˆìƒ\",\"íŒë‹¨\",\"ì§„ë‹¨\",\"ì œì‹œ\",\"ë…¼ì˜\",\"ë°œí‘œ\",\"ê³µê°œ\",\n",
    "\n",
    "    # =========================\n",
    "    # 3. ì£¼ì²´Â·ì¶”ìƒ ì§‘ë‹¨\n",
    "    # =========================\n",
    "    \"í•œêµ­\",\"ì •ë¶€\",\"êµ­ê°€\",\"êµ­ë¯¼\",\"ì¬ê³„\",\"ì—…ê³„\",\n",
    "    \"ê¸°ì—…ë“¤\",\"ì‚¬ëŒ\",\"ì¡°ì§\",\"ë‹¹êµ­\",\"ê´€ê³„ì\",\"ì „ë¬¸ê°€\",\n",
    "    \"ê¸°ê´€\",\"í˜‘íšŒ\",\"ë‹¨ì²´\",\"ì‚¬ì—…\",\"ë¶€ë¬¸\",\n",
    "\n",
    "    # =========================\n",
    "    # 4. ì‹œì Â·ì‹œê°„ í‘œí˜„\n",
    "    # =========================\n",
    "    \"ì˜¬í•´\",\"ë‚´ë…„\",\"ì‘ë…„\",\"ìµœê·¼\",\"í–¥í›„\",\"í˜„ì¬\",\n",
    "    \"ì´ë‚ \",\"ì´í›„\",\"ì´ì „\",\"ë‹¹ì‹œ\",\"ì´ë²ˆ\",\"ì§€ë‚œí•´\",\n",
    "    \"ìƒë°˜ê¸°\",\"í•˜ë°˜ê¸°\",\"ì—°ê°„\",\"ë¶„ê¸°\",\"ì›”ê°„\",\"ì¤‘ì¥ê¸°\",\n",
    "\n",
    "    # =========================\n",
    "    # 5. ê¸°ì‚¬ ë©”íƒ€ / ì¶œì²˜\n",
    "    # =========================\n",
    "    \"ì¸í„°ë·°\",\"ê¸°íš\",\"íŠ¹ë³„\",\"ë³´ë„\",\"ë‰´ìŠ¤\",\n",
    "    \"ê¸°ì\",\"ì·¨ì¬\",\"ì‚¬ì§„\",\"ì œê³µ\",\"ì—°í•©ë‰´ìŠ¤\",\"ë¡œì´í„°\",\n",
    "\n",
    "    # =========================\n",
    "    # 6. ë„ˆë¬´ í¬ê´„ì ì¸ í‰ê°€Â·ìˆ˜ì‹ì–´\n",
    "    # =========================\n",
    "    \"ì£¼ìš”\",\"í•µì‹¬\",\"ë‹¤ì–‘í•œ\",\"ìƒˆë¡œìš´\",\"ë†’ì€\",\"ë‚®ì€\",\n",
    "    \"í°\",\"ì‘ì€\",\"ì¼ë¶€\",\"ë‹¤ìˆ˜\",\"ì—¬ëŸ¬\",\n",
    "    \"ì „ë°˜ì \",\"ëŒ€í‘œì \",\"ì¼ë°˜ì \",\"ì¤‘ìš”\",\"í•„ìš”\",\n",
    "\n",
    "    # =========================\n",
    "    # êµ­ê°€ëª…\n",
    "    # =========================\n",
    "\n",
    "    \"í•œêµ­\",\"ë¯¸êµ­\",\"ì¤‘êµ­\",\"ì¼ë³¸\",\"ìœ ëŸ½\",\"eu\",\n",
    "\n",
    "    # ì•„ì‹œì•„\n",
    "    \"ì•„ì‹œì•„\",\"ë™ë‚¨ì•„\",\"ë² íŠ¸ë‚¨\",\"ì¸ë„\",\"ëŒ€ë§Œ\",\n",
    "\n",
    "    # ìœ ëŸ½ ì£¼ìš”êµ­\n",
    "    \"ë…ì¼\",\"í”„ë‘ìŠ¤\",\"ì˜êµ­\",\"ì´íƒˆë¦¬ì•„\",\"ìŠ¤í˜ì¸\",\n",
    "\n",
    "    # ê¸°íƒ€ ìì£¼ ë“±ì¥\n",
    "    \"ëŸ¬ì‹œì•„\",\"ìš°í¬ë¼ì´ë‚˜\",\"ì¤‘ë™\",\"ì•„í”„ë¦¬ì¹´\",\n",
    "\n",
    "    # ì§€ì—­ í‘œí˜„\n",
    "    \"ê¸€ë¡œë²Œ\",\"ì „ì„¸ê³„\",\"í•´ì™¸\",\"êµ­ë‚´\"\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# ê²€ìƒ‰ì–´ ì˜ë¯¸ ì¤‘ë³µ ì œê±°ìš©\n",
    "QUERY_SEMANTIC_STOPWORDS = {\n",
    "    \"ì´ì°¨ì „ì§€\": {\"ì´ì°¨ì „ì§€\", \"2ì°¨ì „ì§€\", \"ì´ì°¨\", \"ì „ì§€\"},\n",
    "    \"ë°˜ë„ì²´\": {\"ë°˜ë„ì²´\", \"ì¹©\", \"chip\"},\n",
    "    \"ë””ìŠ¤í”Œë ˆì´\": {\"ë””ìŠ¤í”Œë ˆì´\", \"íŒ¨ë„\"},\n",
    "}\n",
    "\n",
    "# ì‚°ì—… ì—°ê´€ ê¸°ì—… (ğŸ”¥ ì „ì—­ ì œê±° ëŒ€ìƒ)\n",
    "RELATED_COMPANIES = {\n",
    "    \"ë°˜ë„ì²´\": [\"ì‚¼ì„±\", \"ì‚¼ì„±ì „ì\", \"sk\", \"skí•˜ì´ë‹‰ìŠ¤\", \"tsmc\", \"ì¸í…”\"],\n",
    "    \"ë””ìŠ¤í”Œë ˆì´\": [\"lg\", \"lgë””ìŠ¤í”Œë ˆì´\", \"ì‚¼ì„±ë””ìŠ¤í”Œë ˆì´\"],\n",
    "    \"ì´ì°¨ì „ì§€\": [\"lgì—ë„ˆì§€ì†”ë£¨ì…˜\", \"ì‚¼ì„±sdi\", \"skì˜¨\", \"catl\"],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 5. ê²€ìƒ‰ì–´ íŒŒì‹± (ë‹¨ì¼ ê¸°ì¤€)\n",
    "#############################################\n",
    "def parse_search_word(search_word):\n",
    "    if \"_\" in search_word:\n",
    "        parts = search_word.split(\"_\")\n",
    "        return parts[0], parts[1]\n",
    "\n",
    "    for industry in QUERY_SEMANTIC_STOPWORDS.keys():\n",
    "        if industry in search_word:\n",
    "            return industry, search_word.replace(industry, \"\")\n",
    "\n",
    "    return search_word, \"\"\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 6. íŒ¨í„´ ì¶”ì¶œ\n",
    "#############################################\n",
    "\n",
    "def extract_all_query_patterns(search_word):\n",
    "    \"\"\"\n",
    "    ê²€ìƒ‰ì–´ ìì²´ + ì˜ë¯¸ì ìœ¼ë¡œ ê°™ì€ ë‹¨ì–´ ì „ë¶€ ì œê±°\n",
    "    \"\"\"\n",
    "    industry, keyword = parse_search_word(search_word)\n",
    "\n",
    "    patterns = {\n",
    "        normalize_token(industry),\n",
    "        normalize_token(keyword)\n",
    "    }\n",
    "\n",
    "    patterns |= {\n",
    "        normalize_token(w)\n",
    "        for w in QUERY_SEMANTIC_STOPWORDS.get(industry, set())\n",
    "    }\n",
    "\n",
    "    return patterns\n",
    "\n",
    "\n",
    "def extract_all_company_patterns():\n",
    "    \"\"\"\n",
    "    ì‚°ì—… êµ¬ë¶„ ì—†ì´ ëª¨ë“  ì—°ê´€ ê¸°ì—… ì „ë¶€ ì œê±°\n",
    "    \"\"\"\n",
    "    patterns = set()\n",
    "    for companies in RELATED_COMPANIES.values():\n",
    "        for c in companies:\n",
    "            patterns.add(normalize_token(c))\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#############################################\n",
    "# 7. ğŸ”¥ í•µì‹¬ í‚¤ì›Œë“œ ì •ì œ í•¨ìˆ˜\n",
    "#############################################\n",
    "def clean_keywords(keyword_series, search_word):\n",
    "    base_stopwords = {normalize_token(w) for w in BASE_STOPWORDS}\n",
    "    query_patterns = extract_all_query_patterns(search_word)\n",
    "    company_patterns = extract_all_company_patterns()\n",
    "\n",
    "    cleaned = []\n",
    "\n",
    "    for text in keyword_series:\n",
    "        if not isinstance(text, str):\n",
    "            continue\n",
    "\n",
    "        for w in text.split(\",\"):\n",
    "            token = normalize_token(w)\n",
    "\n",
    "            # 1ï¸âƒ£ ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ ì œê±°\n",
    "            if len(token) <= 1:\n",
    "                continue\n",
    "\n",
    "            # 2ï¸âƒ£ ê¸°ë³¸ ë¶ˆìš©ì–´ ì œê±°\n",
    "            if token in base_stopwords:\n",
    "                continue\n",
    "\n",
    "            # 3ï¸âƒ£ ê²€ìƒ‰ì–´ + ì˜ë¯¸ ì¤‘ë³µ ì œê±°\n",
    "            if any(p in token for p in query_patterns):\n",
    "                continue\n",
    "\n",
    "            # 4ï¸âƒ£ ê²€ìƒ‰ì–´ ì—°ê´€ ê¸°ì—… ì œê±° (ğŸ”¥ í•µì‹¬)\n",
    "            if any(c in token for c in company_patterns):\n",
    "                continue\n",
    "\n",
    "            cleaned.append(token)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 8. ì›Œë“œí´ë¼ìš°ë“œ ì‹¤í–‰\n",
    "#############################################\n",
    "\n",
    "for search_word, group in df.groupby(\"ê²€ìƒ‰ì–´\"):\n",
    "\n",
    "    group = group.dropna(subset=[\"í‚¤ì›Œë“œ\"])\n",
    "\n",
    "    words = clean_keywords(group[\"í‚¤ì›Œë“œ\"], search_word)\n",
    "    if not words:\n",
    "        continue\n",
    "\n",
    "    # -----------------------------\n",
    "    # í‚¤ì›Œë“œ ë¹ˆë„\n",
    "    # -----------------------------\n",
    "    freq = Counter(words)\n",
    "    top_words = dict(freq.most_common(20))\n",
    "\n",
    "    # -----------------------------\n",
    "    # ì›Œë“œí´ë¼ìš°ë“œ\n",
    "    # -----------------------------\n",
    "    wc = WordCloud(\n",
    "        font_path=font_path,\n",
    "        background_color=\"white\",\n",
    "        width=1000,\n",
    "        height=520,\n",
    "        max_words=20,\n",
    "        relative_scaling=0.25,\n",
    "        colormap=\"viridis\"  # ğŸ”¥ ìƒ‰ê° í†µì¼\n",
    "    ).generate_from_frequencies(top_words)\n",
    "\n",
    "    # -----------------------------\n",
    "    # ì‹œê°í™”\n",
    "    # -----------------------------\n",
    "    fig = plt.figure(figsize=(15, 8), facecolor=\"white\")\n",
    "\n",
    "    # ë©”ì¸ íƒ€ì´í‹€\n",
    "    fig.text(\n",
    "        0.05, 0.94,\n",
    "        \"ì‚°ì—…ì „ë§ í˜„í™©\",\n",
    "        fontsize=18,\n",
    "        fontweight=\"bold\",\n",
    "        ha=\"left\",\n",
    "        color=\"#222222\"\n",
    "    )\n",
    "\n",
    "    # ì„œë¸Œ íƒ€ì´í‹€ (ê²€ìƒ‰ì–´)\n",
    "    fig.text(\n",
    "        0.05, 0.90,\n",
    "        format_search_query(search_word),\n",
    "        fontsize=12,\n",
    "        ha=\"left\",\n",
    "        color=\"#555555\"\n",
    "    )\n",
    "\n",
    "    # êµ¬ë¶„ì„ \n",
    "    fig.lines.append(\n",
    "        plt.Line2D(\n",
    "            [0.05, 0.95], [0.875, 0.875],\n",
    "            linewidth=1,\n",
    "            color=\"#DDDDDD\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # ì›Œë“œí´ë¼ìš°ë“œ ì˜ì—­\n",
    "    ax = fig.add_axes([0.05, 0.10, 0.90, 0.72])\n",
    "    ax.imshow(wc)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### ê°ì„±ë¶„ì„ í¬í•¨ \n",
    "- WC ì—ëŠ” ëª…ì‚¬ë§Œ, ê°ì„±ë¶„ì„í•  ë•Œì—ëŠ” ë™ì‚¬ë¡œë§Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 9. ë™ì‚¬ ê¸°ë°˜ ê°ì„± ì‚¬ì „\n",
    "#############################################\n",
    "\n",
    "POS_VERBS = {\n",
    "    \"ì„±ì¥\", \"í™•ëŒ€\", \"ì¦ê°€\", \"íšŒë³µ\", \"ê°œì„ \",\n",
    "    \"ê°•í™”\", \"í™•ë³´\", \"ìƒìŠ¹\", \"í™•ì¥\", \"í™œì„±í™”\",\n",
    "    \"í˜¸ì¡°\", \"ì§„ì „\", \"ê°œí¸\", \"ë„ì•½\", \"ì•ˆì •\",\n",
    "    \"ì„±ê³µ\", \"í™•ì‚°\", \"ì •ì°©\", \"ì„ ë„\", \"ì´‰ì§„\"\n",
    "}\n",
    "\n",
    "\n",
    "NEG_VERBS = {\n",
    "    \"ê°ì†Œ\", \"ë¶€ì§„\", \"ìœ„ì¶•\", \"ë‘”í™”\", \"í•˜ë½\",\n",
    "    \"ì•…í™”\", \"ì¶•ì†Œ\", \"ì •ì²´\", \"ë¦¬ìŠ¤í¬\", \"ë¶ˆí™•ì‹¤\",\n",
    "    \"ì°¨ì§ˆ\", \"ì§€ì—°\", \"ê°í‡´\", \"ì¹¨ì²´\", \"ë¶€ë‹´\",\n",
    "    \"ë¶ˆì•ˆ\", \"ì•…ì¬\", \"ì œì•½\", \"í›„í‡´\", \"ì†ì‹¤\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 10. í‚¤ì›Œë“œ ë¬¸ìì—´ì—ì„œ ë™ì‚¬ë§Œ ì¶”ì¶œ\n",
    "#############################################\n",
    "\n",
    "def extract_verbs(keyword_series):\n",
    "    verbs = []\n",
    "\n",
    "    pos_verbs = {normalize_token(v) for v in POS_VERBS}\n",
    "    neg_verbs = {normalize_token(v) for v in NEG_VERBS}\n",
    "\n",
    "    for text in keyword_series:\n",
    "        if not isinstance(text, str):\n",
    "            continue\n",
    "\n",
    "        for w in text.split(\",\"):\n",
    "            token = normalize_token(w)\n",
    "\n",
    "            if token in pos_verbs or token in neg_verbs:\n",
    "                verbs.append(token)\n",
    "\n",
    "    return verbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 11. ë™ì‚¬ ê¸°ë°˜ ê°ì„± ì ìˆ˜ ê³„ì‚°\n",
    "#############################################\n",
    "\n",
    "def calculate_sentiment_from_verbs(verbs):\n",
    "    if not verbs:\n",
    "        return 0.0\n",
    "\n",
    "    score = 0\n",
    "    for v in verbs:\n",
    "        if v in POS_VERBS:\n",
    "            score += 1\n",
    "        elif v in NEG_VERBS:\n",
    "            score -= 1\n",
    "\n",
    "    # í‰ê·  ê°ì„± ì ìˆ˜\n",
    "    return round(score / len(verbs), 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "- í‰ê·  ê°ì„± ì ìˆ˜\n",
    "    - [ì •ì˜] (ê¸ì • ë™ì‚¬ ìˆ˜ âˆ’ ë¶€ì • ë™ì‚¬ ìˆ˜) Ã· ì „ì²´ ë™ì‚¬ ìˆ˜\n",
    "\n",
    "- ë²”ìœ„: -1 ~ +1\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 13. ê¸ì • / ë¶€ì • ë™ì‚¬ Top ë¦¬ìŠ¤íŠ¸\n",
    "#############################################\n",
    "\n",
    "def get_top_verbs(verbs, top_n=5):\n",
    "    pos = Counter(v for v in verbs if v in POS_VERBS)\n",
    "    neg = Counter(v for v in verbs if v in NEG_VERBS)\n",
    "\n",
    "    return (\n",
    "        pos.most_common(top_n),\n",
    "        neg.most_common(top_n)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_top_positive_verbs(verbs, top_n=5):\n",
    "    pos_verbs = {normalize_token(v) for v in POS_VERBS}\n",
    "    pos_only = [v for v in verbs if v in pos_verbs]\n",
    "    return Counter(pos_only).most_common(top_n)\n",
    "\n",
    "def get_top_negative_verbs(verbs, top_n=5):\n",
    "    neg_verbs = {normalize_token(v) for v in NEG_VERBS}\n",
    "    neg_only = [v for v in verbs if v in neg_verbs]\n",
    "    return Counter(neg_only).most_common(top_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_verbs(keyword_series):\n",
    "    verbs = []\n",
    "\n",
    "    pos_verbs = {normalize_token(v) for v in POS_VERBS}\n",
    "    neg_verbs = {normalize_token(v) for v in NEG_VERBS}\n",
    "\n",
    "    for text in keyword_series:\n",
    "        if not isinstance(text, str):\n",
    "            continue\n",
    "\n",
    "        for w in text.split(\",\"):\n",
    "            token = normalize_token(w)\n",
    "\n",
    "            if token in pos_verbs or token in neg_verbs:\n",
    "                verbs.append(token)\n",
    "\n",
    "    return verbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_summary(verbs):\n",
    "    pos = sum(1 for v in verbs if v in POS_VERBS)\n",
    "    neg = sum(1 for v in verbs if v in NEG_VERBS)\n",
    "    total = pos + neg\n",
    "\n",
    "    if total == 0:\n",
    "        return 0.0, 0.0, pos, neg\n",
    "\n",
    "    avg_score = (pos - neg) / total\n",
    "    pos_ratio = pos / total\n",
    "\n",
    "    return round(avg_score, 2), round(pos_ratio, 2), pos, neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def sentiment_confidence(verbs):\n",
    "    \"\"\"\n",
    "    ê°ì„± ì ìˆ˜ ì‹ ë¢°ë„ íŒë‹¨\n",
    "    ê¸°ì¤€:\n",
    "    1) ë™ì‚¬ ìˆ˜\n",
    "    2) ê¸/ë¶€ì • ë¹„ìœ¨ í¸í–¥\n",
    "    3) ìƒìœ„ ë™ì‚¬ ì ë¦¼\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(verbs)\n",
    "\n",
    "    # â‘  í‘œë³¸ ìˆ˜ ë¶€ì¡±\n",
    "    if n < 5:\n",
    "        return \"ë‚®ìŒ\"\n",
    "\n",
    "    # ê¸/ë¶€ì • ê°œìˆ˜\n",
    "    pos_set = {normalize_token(v) for v in POS_VERBS}\n",
    "    neg_set = {normalize_token(v) for v in NEG_VERBS}\n",
    "\n",
    "    pos_cnt = sum(1 for v in verbs if v in pos_set)\n",
    "    neg_cnt = sum(1 for v in verbs if v in neg_set)\n",
    "\n",
    "    total = pos_cnt + neg_cnt\n",
    "    if total == 0:\n",
    "        return \"ë‚®ìŒ\"\n",
    "\n",
    "    pos_ratio = pos_cnt / total\n",
    "\n",
    "    # â‘¡ ë°©í–¥ì„± ë¶ˆëª…í™• (ë„ˆë¬´ ì„ì—¬ ìˆìŒ)\n",
    "    if 0.4 <= pos_ratio <= 0.6:\n",
    "        base_conf = \"ë³´í†µ\"\n",
    "    else:\n",
    "        base_conf = \"ë†’ìŒ\"\n",
    "\n",
    "    # â‘¢ íŠ¹ì • ë™ì‚¬ ì ë¦¼ ì²´í¬\n",
    "    freq = Counter(verbs)\n",
    "    top_ratio = freq.most_common(1)[0][1] / n\n",
    "\n",
    "    if top_ratio > 0.6:\n",
    "        # í•œ ë‹¨ì–´ì— ê³¼ë„í•˜ê²Œ ì˜ì¡´\n",
    "        return \"ë³´í†µ\" if base_conf == \"ë†’ìŒ\" else \"ë‚®ìŒ\"\n",
    "\n",
    "    return base_conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 7. ğŸ”¥ í•µì‹¬ í‚¤ì›Œë“œ ì •ì œ í•¨ìˆ˜ (ëª…ì‚¬ìš©)\n",
    "#############################################\n",
    "def clean_keywords(keyword_series, search_word):\n",
    "    base_stopwords = {normalize_token(w) for w in BASE_STOPWORDS}\n",
    "    query_patterns = extract_all_query_patterns(search_word)\n",
    "    company_patterns = extract_all_company_patterns()\n",
    "\n",
    "    # ğŸ”¥ ë™ì‚¬ ì‚¬ì „ (ì›Œë“œí´ë¼ìš°ë“œì—ì„œëŠ” ì œê±°)\n",
    "    verb_patterns = {normalize_token(v) for v in POS_VERBS | NEG_VERBS}\n",
    "\n",
    "    cleaned = []\n",
    "\n",
    "    for text in keyword_series:\n",
    "        if not isinstance(text, str):\n",
    "            continue\n",
    "\n",
    "        for w in text.split(\",\"):\n",
    "            token = normalize_token(w)\n",
    "\n",
    "            # 1ï¸âƒ£ ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ ì œê±°\n",
    "            if len(token) <= 1:\n",
    "                continue\n",
    "\n",
    "            # 2ï¸âƒ£ ê¸°ë³¸ ë¶ˆìš©ì–´\n",
    "            if token in base_stopwords:\n",
    "                continue\n",
    "\n",
    "            # 3ï¸âƒ£ ê²€ìƒ‰ì–´/ì˜ë¯¸ ì¤‘ë³µ ì œê±°\n",
    "            if any(p in token for p in query_patterns):\n",
    "                continue\n",
    "\n",
    "            # 4ï¸âƒ£ ê¸°ì—…ëª… ì œê±°\n",
    "            if any(c in token for c in company_patterns):\n",
    "                continue\n",
    "\n",
    "            # ğŸ”¥ 5ï¸âƒ£ ë™ì‚¬ ì œê±° (í•µì‹¬)\n",
    "            if token in verb_patterns:\n",
    "                continue\n",
    "\n",
    "            cleaned.append(token)\n",
    "\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 12. ì›Œë“œí´ë¼ìš°ë“œ + í‰ê·  ê°ì„± ì ìˆ˜ UI\n",
    "#############################################\n",
    "\n",
    "for search_word, group in df.groupby(\"ê²€ìƒ‰ì–´\"):\n",
    "\n",
    "    group = group.dropna(subset=[\"í‚¤ì›Œë“œ\"])\n",
    "\n",
    "    # -----------------------------\n",
    "    # (1) ëª…ì‚¬ â†’ ì›Œë“œí´ë¼ìš°ë“œ\n",
    "    # -----------------------------\n",
    "    words = clean_keywords(group[\"í‚¤ì›Œë“œ\"], search_word)\n",
    "    if not words:\n",
    "        continue\n",
    "\n",
    "    freq = Counter(words)\n",
    "    top_words = dict(freq.most_common(20))\n",
    "\n",
    "    # wc = WordCloud(\n",
    "    #     font_path=font_path,\n",
    "    #     background_color=\"white\",\n",
    "    #     width=1000,\n",
    "    #     height=520,\n",
    "    #     max_words=20,\n",
    "    #     relative_scaling=0.25,\n",
    "    #     colormap=\"cividis\"\n",
    "    # ).generate_from_frequencies(top_words)\n",
    "\n",
    "    wc = WordCloud(\n",
    "        font_path=font_path,\n",
    "        background_color=\"white\",\n",
    "        width=1000,\n",
    "        height=520,\n",
    "        max_words=20,\n",
    "\n",
    "        # ğŸ”¥ spacing & ê°€ë…ì„± í•µì‹¬\n",
    "        margin=50,\n",
    "        relative_scaling=0.25,\n",
    "        prefer_horizontal=0.95,\n",
    "        max_font_size=150,\n",
    "\n",
    "        # ìƒ‰ê°\n",
    "        colormap=\"magma\"\n",
    "    ).generate_from_frequencies(top_words)\n",
    "\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # (2) ë™ì‚¬ â†’ ê°ì„± ì ìˆ˜\n",
    "    # -----------------------------\n",
    "    verbs = extract_verbs(group[\"í‚¤ì›Œë“œ\"])\n",
    "\n",
    "    avg_sentiment, pos_ratio, pos_cnt, neg_cnt = sentiment_summary(verbs)\n",
    "    confidence = sentiment_confidence(verbs)\n",
    "\n",
    "    pos_top = get_top_positive_verbs(verbs, top_n=5)\n",
    "    neg_top = get_top_negative_verbs(verbs, top_n=5)\n",
    "\n",
    "    # -----------------------------\n",
    "    # (3) ì‹œê°í™”\n",
    "    # -----------------------------\n",
    "    fig = plt.figure(figsize=(16, 8), facecolor=\"white\")\n",
    "\n",
    "    # íƒ€ì´í‹€\n",
    "    fig.text(\n",
    "        0.05, 0.94,\n",
    "        \"ì‚°ì—…ì „ë§ í˜„í™©\",\n",
    "        fontsize=18,\n",
    "        fontweight=\"bold\",\n",
    "        ha=\"left\"\n",
    "    )\n",
    "\n",
    "    # ê²€ìƒ‰ì–´\n",
    "    fig.text(\n",
    "        0.05, 0.90,\n",
    "        format_search_query(search_word),\n",
    "        fontsize=12,\n",
    "        ha=\"left\",\n",
    "        color=\"#555555\"\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # ì›Œë“œí´ë¼ìš°ë“œ (ì¢Œì¸¡)\n",
    "    # -----------------------------\n",
    "    ax_wc = fig.add_axes([0.05, 0.10, 0.68, 0.75])\n",
    "    ax_wc.imshow(wc)\n",
    "    ax_wc.axis(\"off\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # ê°ì„± ì¹´ë“œ (ìš°ì¸¡)\n",
    "    # -----------------------------\n",
    "    ax_card = fig.add_axes([0.75, 0.18, 0.22, 0.62])\n",
    "    ax_card.set_xlim(0, 1)\n",
    "    ax_card.set_ylim(0, 1)\n",
    "    ax_card.axis(\"off\")\n",
    "\n",
    "    sentiment_color = (\n",
    "        \"#00008B\" if avg_sentiment > 0\n",
    "        else \"#e74c3c\" if avg_sentiment < 0\n",
    "        else \"#95a5a6\"\n",
    "    )\n",
    "\n",
    "    # =============================\n",
    "    # (1) í‰ê·  ê°ì„± ì ìˆ˜\n",
    "    # =============================\n",
    "    ax_card.text(\n",
    "        0.5, 0.92,\n",
    "        \"í‰ê·  ê°ì„± ì ìˆ˜\",\n",
    "        fontsize=11,\n",
    "        ha=\"center\",\n",
    "        color=\"#333333\"\n",
    "    )\n",
    "\n",
    "    ax_card.text(\n",
    "        0.5, 0.82,\n",
    "        f\"{avg_sentiment:+.2f}\",\n",
    "        fontsize=38,\n",
    "        fontweight=\"bold\",\n",
    "        ha=\"center\",\n",
    "        color=sentiment_color\n",
    "    )\n",
    "\n",
    "    ax_card.text(\n",
    "        0.5, 0.72,\n",
    "        f\"ì‹ ë¢°ë„: {confidence}\",\n",
    "        fontsize=10,\n",
    "        ha=\"center\",\n",
    "        color=\"#777777\"\n",
    "    )\n",
    "\n",
    "    # êµ¬ë¶„ì„ \n",
    "    ax_card.plot([0.05, 0.95], [0.66, 0.66], color=\"#DDDDDD\", lw=1)\n",
    "\n",
    "    # =============================\n",
    "    # (2) ê¸ì • / ë¶€ì • ë™ì‚¬ 2ì»¬ëŸ¼\n",
    "    # =============================\n",
    "    # ğŸŸ¢ ê¸ì •\n",
    "    ax_card.text(0.25, 0.60, \"[+] TOP 5\",\n",
    "                fontsize=11, ha=\"center\",\n",
    "                fontweight=\"bold\", color=\"#27ae60\")\n",
    "\n",
    "    y_pos = 0.54\n",
    "    for verb, cnt in pos_top:\n",
    "        ax_card.text(\n",
    "            0.25, y_pos,\n",
    "            f\"{verb} ({cnt})\",\n",
    "            fontsize=10,\n",
    "            ha=\"center\",\n",
    "            color=\"#333333\"\n",
    "        )\n",
    "        y_pos -= 0.06\n",
    "\n",
    "\n",
    "    # ğŸ”´ ë¶€ì •\n",
    "    ax_card.text(0.75, 0.60, \"[-] TOP 5\",\n",
    "                fontsize=11, ha=\"center\",\n",
    "                fontweight=\"bold\", color=\"#c0392b\")\n",
    "\n",
    "    y_neg = 0.54\n",
    "    for verb, cnt in neg_top:\n",
    "        ax_card.text(\n",
    "            0.75, y_neg,\n",
    "            f\"{verb} ({cnt})\",\n",
    "            fontsize=10,\n",
    "            ha=\"center\",\n",
    "            color=\"#333333\"\n",
    "        )\n",
    "        y_neg -= 0.06\n",
    "\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
