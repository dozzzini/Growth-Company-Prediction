{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Í∏∞ÏóÖ ÏÑ±Ïû• ÏòàÏ∏° Î™®Îç∏ÎßÅ - Feature Í∑∏Î£πÎ≥Ñ Î∂ÑÏÑù\n",
    "## Ï£ºÏöî ÏàòÏ†ï ÏÇ¨Ìï≠\n",
    "1. ‚úÖ **4Í∞ÄÏßÄ Î™®Îç∏ Î™ÖÌôïÌûà Ï†ïÏùò**: LogReg, RF, XGB, LGBM\n",
    "2. ‚úÖ **ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù Ï∂îÍ∞Ä**: GridSearchCV ÏÇ¨Ïö©\n",
    "3. ‚úÖ **MLflow Î°úÍπÖ Íµ¨ÌòÑ**: Î™®Îì† Ïã§Ìóò ÏûêÎèô Í∏∞Î°ù\n",
    "4. ‚úÖ **5Í∞ÄÏßÄ ÌèâÍ∞ÄÏßÄÌëú ÏôÑÎπÑ**: roc_auc, top20_precision, top50_precision, f1, brier_score\n",
    "5. üÜï **Feature Í∑∏Î£πÎ≥Ñ Ïã§Ìóò**: Ïû¨Î¨¥/ÌäπÌóà Îç∞Ïù¥ÌÑ∞ Ï°∞Ìï© ÎπÑÍµê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Í∏∞Î≥∏ ÎùºÏù¥Î∏åÎü¨Î¶¨\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Î™®Îç∏ÎßÅ ÎùºÏù¥Î∏åÎü¨Î¶¨\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ÌäúÎãù Î∞è Í≤ÄÏ¶ù\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "# ÌèâÍ∞Ä ÏßÄÌëú\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    brier_score_loss,\n",
    "    precision_score\n",
    ")\n",
    "\n",
    "# MLflow Î°úÍπÖ\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from datetime import datetime\n",
    "\n",
    "# tqdm Î°úÎî©\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Îç∞Ïù¥ÌÑ∞ Î°úÎìú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "DATA_DIR = Path(\"../modeling_data\")\n",
    "\n",
    "# ÌïôÏäµ/ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "train_df = pd.read_csv(DATA_DIR / \"train_dataset.csv\")\n",
    "test_df = pd.read_csv(DATA_DIR / \"test_dataset.csv\")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. üÜï Feature Í∑∏Î£π Ï†ïÏùò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Feature Í∑∏Î£π Ï†ïÏùò\n",
    "# =========================\n",
    "\n",
    "ID_COLS = ['Í∏∞ÏóÖÎ™Ö', 'Ïó∞ÎèÑ', 'target_year']\n",
    "TARGET_COL = 'target_growth'\n",
    "\n",
    "# 1. Ïû¨Î¨¥ Îç∞Ïù¥ÌÑ∞\n",
    "FIN_COLS = [\n",
    "    'revenue_t1_scaled',\n",
    "    'cagr_2y_scaled',\n",
    "    'growth_recent_scaled',\n",
    "    'growth_acceleration_scaled',\n",
    "    'growth_volatility_scaled',\n",
    "    'operating_margin_scaled',\n",
    "    'capex_intensity_scaled',\n",
    "    'capex_trend_scaled',\n",
    "    'capex_vs_industry_scaled',\n",
    "    'debt_ratio_scaled',\n",
    "    'rnd_intensity_scaled',\n",
    "    'profitable_years'\n",
    "]\n",
    "\n",
    "# 2. ÌäπÌóà ÏàòÏπò Îç∞Ïù¥ÌÑ∞\n",
    "PATENT_NUM_COLS = [\n",
    "    'patent_count_5y',\n",
    "    'patent_count_recent',\n",
    "    'ipc_diversity',\n",
    "    'patent_register_rate',\n",
    "    'patent_citation_total',\n",
    "    'patent_citation_avg',\n",
    "    'patent_citation_age_adj_total',\n",
    "    'patent_citation_age_adj_avg',\n",
    "    'no_patent_flag'\n",
    "]\n",
    "\n",
    "# 3. ÌäπÌóà ÏûÑÎ≤†Îî© Îç∞Ïù¥ÌÑ∞\n",
    "PATENT_EMB_COLS = [col for col in train_df.columns if col.startswith('patent_emb_')]\n",
    "\n",
    "print(\"‚úÖ Feature Í∑∏Î£π Ï†ïÏùò ÏôÑÎ£å:\")\n",
    "print(f\"  - Ïû¨Î¨¥ Îç∞Ïù¥ÌÑ∞: {len(FIN_COLS)}Í∞ú\")\n",
    "print(f\"  - ÌäπÌóà ÏàòÏπò: {len(PATENT_NUM_COLS)}Í∞ú\")\n",
    "print(f\"  - ÌäπÌóà ÏûÑÎ≤†Îî©: {len(PATENT_EMB_COLS)}Í∞ú\")\n",
    "print(f\"  - Ï†ÑÏ≤¥: {len(FIN_COLS) + len(PATENT_NUM_COLS) + len(PATENT_EMB_COLS)}Í∞ú\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Ïã§ÌóòÌï† Feature Ï°∞Ìï© Ï†ïÏùò\n",
    "# =========================\n",
    "\n",
    "FEATURE_COMBINATIONS = {\n",
    "    'FIN_ONLY': {\n",
    "        'features': FIN_COLS,\n",
    "        'description': 'Ïû¨Î¨¥ Îç∞Ïù¥ÌÑ∞Îßå'\n",
    "    },\n",
    "    'PATENT_NUM_ONLY': {\n",
    "        'features': PATENT_NUM_COLS,\n",
    "        'description': 'ÌäπÌóà ÏàòÏπò Îç∞Ïù¥ÌÑ∞Îßå'\n",
    "    },\n",
    "    # 'PATENT_EMB_ONLY': {\n",
    "    #     'features': PATENT_EMB_COLS,\n",
    "    #     'description': 'ÌäπÌóà ÏûÑÎ≤†Îî©Îßå'\n",
    "    # },\n",
    "    'FIN_PATENT_NUM': {\n",
    "        'features': FIN_COLS + PATENT_NUM_COLS,\n",
    "        'description': 'Ïû¨Î¨¥ + ÌäπÌóà ÏàòÏπò'\n",
    "    },\n",
    "    # 'FIN_PATENT_EMB': {\n",
    "    #     'features': FIN_COLS + PATENT_EMB_COLS,\n",
    "    #     'description': 'Ïû¨Î¨¥ + ÌäπÌóà ÏûÑÎ≤†Îî©'\n",
    "    # },\n",
    "    # 'PATENT_NUM_EMB': {\n",
    "    #     'features': PATENT_NUM_COLS + PATENT_EMB_COLS,\n",
    "    #     'description': 'ÌäπÌóà ÏàòÏπò + ÏûÑÎ≤†Îî©'\n",
    "    # },\n",
    "    'ALL': {\n",
    "        'features': FIN_COLS + PATENT_NUM_COLS + PATENT_EMB_COLS,\n",
    "        'description': 'Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ Ïã§ÌóòÌï† Feature Ï°∞Ìï©:\")\n",
    "for name, config in FEATURE_COMBINATIONS.items():\n",
    "    print(f\"  - {name:20s}: {config['description']:20s} ({len(config['features'])}Í∞ú feature)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TargetÍ≥º Î©îÌÉÄ Îç∞Ïù¥ÌÑ∞ Î∂ÑÎ¶¨\n",
    "y_train = train_df[TARGET_COL]\n",
    "y_test = test_df[TARGET_COL]\n",
    "years_train = train_df['Ïó∞ÎèÑ']\n",
    "groups_train = train_df['Í∏∞ÏóÖÎ™Ö']\n",
    "\n",
    "print(f\"\\nTarget distribution (train): {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Target distribution (test): {y_test.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3. ÌèâÍ∞Ä ÏßÄÌëú Ìï®Ïàò Ï†ïÏùò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, y_proba):\n",
    "    \"\"\"\n",
    "    Î™®Îç∏ ÌèâÍ∞Ä Ìï®Ïàò\n",
    "    \"\"\"\n",
    "    # 1. ROC-AUC\n",
    "    roc_auc = roc_auc_score(y_true, y_proba)\n",
    "    \n",
    "    # 2. F1 Score\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # 3. Brier Score\n",
    "    brier = brier_score_loss(y_true, y_proba)\n",
    "    \n",
    "    # 4. Top-20 Precision\n",
    "    top_20_idx = np.argsort(y_proba)[-20:]\n",
    "    top_20_precision = y_true.iloc[top_20_idx].mean() if len(top_20_idx) > 0 else 0.0\n",
    "    \n",
    "    # 5. Top-50 Precision\n",
    "    top_50_idx = np.argsort(y_proba)[-50:]\n",
    "    top_50_precision = y_true.iloc[top_50_idx].mean() if len(top_50_idx) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'roc_auc': roc_auc,\n",
    "        'f1': f1,\n",
    "        'brier': brier,\n",
    "        'top20_precision': top_20_precision,\n",
    "        'top50_precision': top_50_precision\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4. Í≤ÄÏ¶ù Î∞©Î≤ï Ï†ïÏùò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout_validation(model, X, y, years):\n",
    "    train_mask = years <= 2022\n",
    "    val_mask = years == 2023\n",
    "\n",
    "    X_tr = X.loc[train_mask]\n",
    "    y_tr = y.loc[train_mask]\n",
    "    X_val = X.loc[val_mask]\n",
    "    y_val = y.loc[val_mask]\n",
    "\n",
    "    if y_tr.nunique() < 2 or y_val.shape[0] == 0:\n",
    "        return None\n",
    "\n",
    "    model_clone = clone(model)\n",
    "    model_clone.fit(X_tr, y_tr)\n",
    "\n",
    "    y_pred = model_clone.predict(X_val)\n",
    "    y_proba = model_clone.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    return evaluate_model(y_val, y_pred, y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_kfold_validation(model, X, y, groups, n_splits=5):\n",
    "    \"\"\"GroupKFold Validation: Í∏∞ÏóÖ Îã®ÏúÑÎ°ú fold Î∂ÑÌï†\"\"\"\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in gkf.split(X, y, groups):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        model_clone = clone(model)\n",
    "        model_clone.fit(X_tr, y_tr)\n",
    "        \n",
    "        y_pred = model_clone.predict(X_val)\n",
    "        y_proba = model_clone.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        scores.append(evaluate_model(y_val, y_pred, y_proba))\n",
    "\n",
    "    return pd.DataFrame(scores).mean().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_validation(model, X, y, years):\n",
    "    \"\"\"Rolling Window Validation: ÏãúÍ≥ÑÏó¥ ÏàúÏ∞® Í≤ÄÏ¶ù\"\"\"\n",
    "    fold_scores = []\n",
    "\n",
    "    for train_end in sorted(years.unique()):\n",
    "        val_year = train_end + 1\n",
    "\n",
    "        train_mask = years <= train_end\n",
    "        val_mask = years == val_year\n",
    "\n",
    "        X_tr = X.loc[train_mask]\n",
    "        y_tr = y.loc[train_mask]\n",
    "        X_val = X.loc[val_mask]\n",
    "        y_val = y.loc[val_mask]\n",
    "\n",
    "        if X_tr.shape[0] == 0 or X_val.shape[0] == 0:\n",
    "            continue\n",
    "        if y_tr.nunique() < 2:\n",
    "            continue\n",
    "\n",
    "        model_clone = clone(model)\n",
    "        model_clone.fit(X_tr, y_tr)\n",
    "\n",
    "        y_pred = model_clone.predict(X_val)\n",
    "        y_proba = model_clone.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        fold_scores.append(evaluate_model(y_val, y_pred, y_proba))\n",
    "\n",
    "    if len(fold_scores) == 0:\n",
    "        return None\n",
    "\n",
    "    return pd.DataFrame(fold_scores).mean().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 5. Î™®Îç∏ Ï†ïÏùò Î∞è ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∑∏Î¶¨Îìú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4Í∞ÄÏßÄ Î™®Îç∏Í≥º ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∑∏Î¶¨Îìú Ï†ïÏùò\n",
    "\n",
    "models_and_params = {\n",
    "    'LogReg': {\n",
    "        'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'params': {\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear'],\n",
    "            'class_weight': [None, 'balanced']\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'RF': {\n",
    "        'model': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, 30, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'class_weight': [None, 'balanced']\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'XGB': {\n",
    "        'model': XGBClassifier(random_state=42, eval_metric='logloss', n_jobs=-1),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7, 10],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'subsample': [0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "            'scale_pos_weight': [1, 2, 3]\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'LGBM': {\n",
    "        'model': LGBMClassifier(random_state=42, verbose=-1, n_jobs=-1),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7, 10],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'num_leaves': [31, 50, 70],\n",
    "            'subsample': [0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "            'class_weight': [None, 'balanced']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Ï†ïÏùòÎêú Î™®Îç∏:\")\n",
    "for name in models_and_params.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 6. MLflow ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow Ïã§Ìóò ÏÑ§Ï†ï\n",
    "experiment_name = \"Growth_Prediction_Feature_Groups\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"‚úÖ MLflow Ïã§Ìóò ÏÑ§Ï†ï ÏôÑÎ£å: {experiment_name}\")\n",
    "print(f\"   Ï∂îÏ†Å URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 7. üÜï Feature Í∑∏Î£πÎ≥Ñ Ïã§Ìóò Ïã§Ìñâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# üî• ÌÉêÏÉâÏö© Í≤ΩÎüâ Ïã§Ìóò ÏΩîÎìú\n",
    "# - Î™©Ï†Å: Feature Group √ó Model Îπ†Î•∏ Ïä§ÌÅ¨Î¶¨Îãù\n",
    "# =========================================================\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "\n",
    "fast_results = []\n",
    "\n",
    "print(\"üöÄ ÌÉêÏÉâÏö© Í≤ΩÎüâ Ïã§Ìóò ÏãúÏûë\")\n",
    "\n",
    "for feature_group_name, feature_config in tqdm(\n",
    "    FEATURE_COMBINATIONS.items(),\n",
    "    desc=\"üß© Feature Groups\"\n",
    "):\n",
    "    feature_cols = feature_config[\"features\"]\n",
    "    X_train_group = train_df[feature_cols]\n",
    "\n",
    "    for model_name, model_config in tqdm(\n",
    "        models_and_params.items(),\n",
    "        desc=f\"ü§ñ Models ({feature_group_name})\",\n",
    "        leave=False\n",
    "    ):\n",
    "        model = clone(model_config[\"model\"])\n",
    "\n",
    "        # -----------------------------\n",
    "        # üîÅ Rolling ValidationÎßå ÏàòÌñâ\n",
    "        # -----------------------------\n",
    "        rolling_scores = rolling_validation(\n",
    "            model,\n",
    "            X_train_group,\n",
    "            y_train,\n",
    "            years_train\n",
    "        )\n",
    "\n",
    "        if rolling_scores is None:\n",
    "            continue\n",
    "\n",
    "        result_row = {\n",
    "            \"feature_group\": feature_group_name,\n",
    "            \"n_features\": len(feature_cols),\n",
    "            \"model\": model_name,\n",
    "            **rolling_scores\n",
    "        }\n",
    "\n",
    "        fast_results.append(result_row)\n",
    "\n",
    "# Í≤∞Í≥º DataFrame\n",
    "fast_results_df = pd.DataFrame(fast_results)\n",
    "\n",
    "print(\"\\n‚úÖ ÌÉêÏÉâÏö© Ïã§Ìóò ÏôÑÎ£å\")\n",
    "fast_results_df.sort_values(\n",
    "    by=\"top20_precision\", ascending=False\n",
    ").head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
