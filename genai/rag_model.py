"""
RAG ëª¨ë¸ ì‹¤í–‰ ì˜ˆì œ
CSV ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ChromaDBë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.
"""
import os
import sys
from dotenv import load_dotenv
from tqdm import tqdm

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ì—ì„œ)
load_dotenv()

# OpenAI API í‚¤ í™•ì¸
if not os.getenv("OPENAI_API_KEY"):
    print("="*70)
    print("ê²½ê³ : OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("="*70)
    print("\në‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ë°©ë²•ìœ¼ë¡œ API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”:")
    print("1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •:")
    print("   export OPENAI_API_KEY='your-api-key-here'")
    print("\n2. .env íŒŒì¼ ìƒì„± (í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—):")
    print("   OPENAI_API_KEY=your-api-key-here")
    print("="*70)
    sys.exit(1)

# ì§ì ‘ ì‹¤í–‰ ì‹œ ìƒëŒ€ import ì˜¤ë¥˜ ë°©ì§€
try:
    from .data_loader import load_all_data, combine_all_texts
    from .chunking import chunk_text
    from .embedding import create_vector_db
    from .prompt import create_reviewer_prompt
    from .rag_chain import create_rag_chain, run_rag_chain, create_llm
except ImportError:
    # ì§ì ‘ ì‹¤í–‰ ì‹œ (python rag_model.py)
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from genai.data_loader import load_all_data, combine_all_texts
    from genai.chunking import chunk_text
    from genai.embedding import create_vector_db
    from genai.prompt import create_reviewer_prompt
    from genai.rag_chain import create_rag_chain, run_rag_chain, create_llm


def main(sample_size=None, force_recreate=False):
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    
    Args:
        sample_size: í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ í¬ê¸° (Noneì´ë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©)
                    ì˜ˆ: sample_size=1000 -> ì²˜ìŒ 1000ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©
        force_recreate: Trueë©´ ê¸°ì¡´ ë²¡í„° DBë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±
    """
    # ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì •
    base_path = os.path.join(os.path.dirname(__file__), "..", "data")
    patent_file = os.path.join(base_path, "íŠ¹í—ˆì •ë³´_final_v2.csv")
    financial_file = os.path.join(base_path, "ì¬ë¬´ì •ë³´_final_imputed.csv")
    
    # 1. CSV ë°ì´í„° ë¡œë“œ ë° í…ìŠ¤íŠ¸ ë³€í™˜
    print("="*70)
    print("[1ë‹¨ê³„] CSV ë°ì´í„° ë¡œë“œ ë° í…ìŠ¤íŠ¸ ë³€í™˜")
    print("="*70)
    data_dict = load_all_data(patent_file, financial_file)
    
    # ëª¨ë“  í…ìŠ¤íŠ¸ ë° ë©”íƒ€ë°ì´í„° ê²°í•©
    all_texts, all_metadata = combine_all_texts(data_dict)
    print(f"\n  ì´ {len(all_texts)}ê°œì˜ ë¬¸ì„œ ìƒì„± ì™„ë£Œ")
    print(f"  - íŠ¹í—ˆ ì •ë³´: {sum(1 for m in all_metadata if m['type'] == 'patent')}ê±´")
    print(f"  - ì¬ë¬´ ì •ë³´: {sum(1 for m in all_metadata if m['type'] == 'financial')}ê±´")
    print(f"  - í”¼ì¸ìš© 5íšŒ ì´ìƒ íŠ¹í—ˆ: {sum(1 for m in all_metadata if m.get('citation_count', 0) >= 5)}ê±´")
    
    # ìƒ˜í”Œ í¬ê¸° ì ìš© (í…ŒìŠ¤íŠ¸ìš©)
    if sample_size:
        print(f"\n  âš ï¸  í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {sample_size}ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤")
        all_texts = all_texts[:sample_size]
        all_metadata = all_metadata[:sample_size]
    
    # 2. í…ìŠ¤íŠ¸ ì²­í‚¹
    print("\n" + "="*70)
    print("[2ë‹¨ê³„] í…ìŠ¤íŠ¸ ì²­í‚¹")
    print("="*70)
    chunk_size = 1000
    chunk_overlap = 100
    
    # ê° í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹
    chunked_texts = []
    for text in tqdm(all_texts, desc="  í…ìŠ¤íŠ¸ ì²­í‚¹ ì§„í–‰", unit="ë¬¸ì„œ"):
        chunks = chunk_text(text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        chunked_texts.extend(chunks)
    
    print(f"  ì²­í¬ í¬ê¸°: {chunk_size}, ê²¹ì¹¨: {chunk_overlap}")
    print(f"  ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunked_texts)}ê°œ")
    
    # 3. ChromaDB ë²¡í„° DB ìƒì„± ë˜ëŠ” ë¡œë“œ
    print("\n" + "="*70)
    print("[3ë‹¨ê³„] ChromaDB ë²¡í„° DB ìƒì„±/ë¡œë“œ")
    print("="*70)
    persist_directory = os.path.join(os.path.dirname(__file__), "chroma_db")
    
    # ê¸°ì¡´ DB ì¡´ì¬ í™•ì¸
    db_exists = os.path.exists(persist_directory) and os.path.exists(
        os.path.join(persist_directory, "chroma.sqlite3")
    )
    
    if db_exists and not force_recreate:
        print(f"  âœ“ ê¸°ì¡´ ë²¡í„° DB ë°œê²¬!")
        print(f"  âœ“ ì„ë² ë”© ìƒì„±ì„ ìŠ¤í‚µí•˜ê³  ê¸°ì¡´ DBë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        print(f"  ğŸ’¡ ìƒˆë¡œ ìƒì„±í•˜ë ¤ë©´: python rag_model.py --recreate")
    
    # ì²­í‚¹ëœ í…ìŠ¤íŠ¸ì— ëŒ€ì‘í•˜ëŠ” ë©”íƒ€ë°ì´í„° ìƒì„± (ì²­í‚¹ìœ¼ë¡œ ì¸í•´ í™•ì¥)
    chunked_metadata = []
    text_idx = 0
    for text in all_texts:
        chunks = chunk_text(text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        # ê°™ì€ ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ì²­í¬ëŠ” ê°™ì€ ë©”íƒ€ë°ì´í„° ì‚¬ìš©
        for _ in chunks:
            if text_idx < len(all_metadata):
                chunked_metadata.append(all_metadata[text_idx])
        text_idx += 1
    
    print(f"  âœ“ ì²­í‚¹ ì™„ë£Œ: {len(chunked_metadata)}ê°œ ë©”íƒ€ë°ì´í„° ìƒì„±")
    
    vector_db = create_vector_db(
        texts=chunked_texts,
        persist_directory=persist_directory,
        collection_name="company_data",
        force_recreate=force_recreate,
        metadata_list=chunked_metadata
    )
    
    if db_exists and not force_recreate:
        print(f"  âœ“ ë²¡í„° DB ë¡œë“œ ì™„ë£Œ: {persist_directory}")
    else:
        print(f"  âœ“ ë²¡í„° DB ìƒì„± ì™„ë£Œ: {persist_directory}")
        print(f"  âœ“ ì €ì¥ëœ ë¬¸ì„œ ìˆ˜: {len(chunked_texts)}ê°œ")
    
    # 4. LLM ë° í”„ë¡¬í”„íŠ¸ ì„¤ì •
    print("\n" + "="*70)
    print("[4ë‹¨ê³„] LLM ë° í”„ë¡¬í”„íŠ¸ ì„¤ì •")
    print("="*70)
    llm = create_llm(model_name="gpt-4o", temperature=0)
    prompt = create_reviewer_prompt()
    print("  LLM ëª¨ë¸: gpt-4o")
    print("  í”„ë¡¬í”„íŠ¸: ê¸°ìˆ ê¸ˆìœµ ì‹¬ì‚¬ì—­ í˜ë¥´ì†Œë‚˜")
    
    # 5. RAG ì²´ì¸ êµ¬ì¶•
    print("\n" + "="*70)
    print("[5ë‹¨ê³„] RAG ì²´ì¸ êµ¬ì¶•")
    print("="*70)
    qa_chain = create_rag_chain(
        vector_db, 
        llm=llm, 
        prompt=prompt,
        search_kwargs={"k": 5}  # ìƒìœ„ 5ê°œ ë¬¸ì„œ ê²€ìƒ‰
    )
    print("  RAG ì²´ì¸ êµ¬ì¶• ì™„ë£Œ")
    
    # 6. ì‹¤í–‰ ì˜ˆì œ (XGBoost + RAG í†µí•©)
    print("\n" + "="*70)
    print("[6ë‹¨ê³„] XGBoost + RAG í†µí•© ë¶„ì„")
    print("="*70)
    
    company_name = "ë¹„ì¸ ë¡œì…€"
    year = 2023
    use_xgboost = True  # XGBoost í†µí•© ì‚¬ìš© ì—¬ë¶€
    
    print(f"\në¶„ì„ ëŒ€ìƒ íšŒì‚¬: {company_name} ({year}ë…„)")
    
    try:
        from .company_search import search_by_company, format_company_context
        from .xgb_integration import analyze_company_with_xgb_and_rag
    except ImportError:
        sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        from genai.company_search import search_by_company, format_company_context
        from genai.xgb_integration import analyze_company_with_xgb_and_rag
    
    # XGBoost í†µí•© ì‚¬ìš© ì‹œ
    if use_xgboost:
        try:
            print("\n[XGBoost ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰]")
            xgb_result, shap_contributions, question = analyze_company_with_xgb_and_rag(
                company_name=company_name,
                year=year,
                top_n_features=10
            )
            
            if xgb_result is None:
                print("âš ï¸  XGBoost ì˜ˆì¸¡ ì‹¤íŒ¨, ê¸°ë³¸ RAG ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                use_xgboost = False
        except Exception as e:
            print(f"âš ï¸  XGBoost í†µí•© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("âš ï¸  ê¸°ë³¸ RAG ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            use_xgboost = False
    
    # ê¸°ë³¸ RAG ëª¨ë“œ (XGBoost ì—†ì´)
    if not use_xgboost:
        question = f"{company_name}ì˜ ê¸°ìˆ ì  ê°•ì ê³¼ ì¬ë¬´ ì•ˆì •ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì„±ì¥ ê°€ëŠ¥ì„±ì„ í‰ê°€í•´ì£¼ì„¸ìš”."
    
    # íŠ¹í—ˆ + ì¬ë¬´ ê²€ìƒ‰
    patent_docs, financial_docs = search_by_company(vector_db, company_name, k=50)
    print(f"\nê²€ìƒ‰ ê²°ê³¼: íŠ¹í—ˆ {len(patent_docs)}ê±´, ì¬ë¬´ {len(financial_docs)}ê±´\n")
    
    # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    context = format_company_context(patent_docs, financial_docs)
    
    print(f"ì§ˆë¬¸:\n{question}\n")
    
    # í”„ë¡¬í”„íŠ¸ ì§ì ‘ ìƒì„±
    prompt_template = prompt
    full_prompt = prompt_template.format(context=context, question=question)
    
    print("="*70)
    print("ìƒì„±ëœ ë³´ê³ ì„œ:")
    print("="*70)
    report = llm.invoke(full_prompt).content
    print(report)
    print("="*70)
    
    # XGBoost ê²°ê³¼ ìš”ì•½ ì¶œë ¥ (ì‚¬ìš© ì‹œ)
    if use_xgboost and xgb_result:
        print("\n" + "="*70)
        print("[XGBoost ëª¨ë¸ ì˜ˆì¸¡ ìš”ì•½]")
        print("="*70)
        print(f"  ê¸°ì—…ëª…: {xgb_result['company_name']}")
        print(f"  ì„±ì¥ í™•ë¥ : {xgb_result['growth_probability']:.2%}")
        print(f"  ì˜ˆì¸¡: {'âœ… ìƒìœ„ 30% ì„±ì¥ ì˜ˆìƒ' if xgb_result['predicted_growth'] == 1 else 'âš ï¸  í•˜ìœ„ 70%'}")
        if shap_contributions:
            print(f"\n  ì£¼ìš” ê¸°ì—¬ í”¼ì²˜ (SHAP Top 5):")
            for i, contrib in enumerate(shap_contributions[:5], 1):
                impact_icon = "ğŸ“ˆ" if contrib['impact'] == "ê¸ì •ì " else "ğŸ“‰"
                print(f"    {i}. {impact_icon} {contrib['feature']}: {contrib['impact']} (SHAP: {contrib['shap_value']:+.4f})")
        print("="*70)


if __name__ == "__main__":
    import sys
    
    # ì»¤ë§¨ë“œ ë¼ì¸ ì¸ì íŒŒì‹±
    # ì˜ˆ: python rag_model.py 1000  -> 1000ê°œë§Œ ì²˜ë¦¬
    # ì˜ˆ: python rag_model.py --recreate  -> ê¸°ì¡´ DB ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±
    # ì˜ˆ: python rag_model.py 1000 --recreate  -> 1000ê°œë¡œ ì¬ìƒì„±
    sample_size = None
    force_recreate = False
    
    for arg in sys.argv[1:]:
        if arg == "--recreate":
            force_recreate = True
            print("âš ï¸  ë²¡í„° DB ì¬ìƒì„± ëª¨ë“œ")
        else:
            try:
                sample_size = int(arg)
                print(f"ìƒ˜í”Œ í¬ê¸°: {sample_size}ê°œ ë¬¸ì„œ")
            except ValueError:
                print(f"ì•Œ ìˆ˜ ì—†ëŠ” ì¸ì: {arg}")
    
    main(sample_size=sample_size, force_recreate=force_recreate)